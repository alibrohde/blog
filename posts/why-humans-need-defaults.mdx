---
title: "Why Humans Need Defaults"
date: "2025-12-01"
subtitle: "We cannot build what we cannot envision, and most of us are extremely bad at envisioning"
thumbnail: "https://beehiiv-images-production.s3.amazonaws.com/uploads/publication/logo/ac9b8eee-a626-4dc7-8863-a2998699920c/ARJ_Logo.jpeg"
---

Like every other VC, I rely on the AI notetaker Granola. It’s excellent at transcripts, summaries, and structured capture. But the thing that actually changed my behavior wasn’t a new capability. It was a single built-in prompt sitting in the sidebar: “What did I miss?”

I could probably ask the transcript questions manually. I’m not even sure — because I never did. The moment that behavior was turned into a visible button, I started using it constantly.

That pattern shows up clearly in the data. Granola’s most-used “recipe” by far is “What did I miss?”, with over 116,000 uses. “Make me sound smart” and “Suggest topics” follow behind. People aren’t asking for power. They’re asking for help knowing what to do next.

This reveals a broader truth about AI adoption: we don’t adopt tools because of what they can do in theory. We adopt them because we can immediately see how they fit into our workflow today.

That’s the core challenge facing OpenAI, Anthropic, and every company shipping general-purpose AI. The bottleneck isn’t capability. It’s helping users form intent, structure a workflow, and recognize a concrete use case before cognitive friction sets in.

We’ve seen this movie before. Airtable was so flexible that most users didn’t know what to build. Adoption didn’t accelerate until the company shipped templates and examples that made the abstract concrete. Notion, Zapier, and others followed the same path. Flexibility without structure slows adoption.

The next frontier of AI likely looks less like “AI that executes” and more like “AI that proposes.”

Systems that say: I notice you do X repeatedly. Here’s a workflow that would save you Y hours per week. Want me to build it?

But this is harder than it sounds. Inferring intent from behavior is noisy. Observing what people do captures their current patterns, not wat they wish they could do but haven’t articulated yet. And false positives kill trust quickly. Suggest the wrong workflow often enough and users stop paying attention altogether.

So the real question isn’t whether AI can propose. It’s whether it can propose well enough to be useful without being annoying.

If someone closes that gap — reliably surfacing the right suggestion at the right moment — it won’t just improve adoption. It will define the next phase of human-AI interaction.
